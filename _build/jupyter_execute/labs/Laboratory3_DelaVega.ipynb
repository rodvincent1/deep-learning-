{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139e3404",
   "metadata": {
    "id": "139e3404"
   },
   "source": [
    "\n",
    "# Laboratory 3  \n",
    "**Name:** Rod Vincent Dela Vega\n",
    "\n",
    "**Section:** DS4A\n",
    "\n",
    "**Task:** Perform a forward and backward propagation in Python using the inputs from Laboratory Task 2\n",
    "\n",
    "---\n",
    "\n",
    "### **Objective**\n",
    "To perform both forward and backward propagation in a simple neural network using ReLU as the activation function and update the weights accordingly.\n",
    "\n",
    "### **Given Data**\n",
    "```python\n",
    "x = np.array([1, 0, 1])\n",
    "y = np.array([1])\n",
    "lr = 0.001\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2af388",
   "metadata": {
    "id": "2f2af388"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Input and target\n",
    "x = np.array([1, 0, 1]).reshape(3, 1)\n",
    "y = np.array([1]).reshape(1, 1)\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Initialize weights (same as Laboratory 2)\n",
    "W1 = np.array([[0.2, -0.3],\n",
    "               [0.4,  0.1],\n",
    "               [-0.5,  0.2]])\n",
    "\n",
    "W2 = np.array([[-0.3, -0.2]])\n",
    "\n",
    "# Biases\n",
    "theta = np.array([[-0.4], [0.2], [0.1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20699e63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20699e63",
    "outputId": "60121801-be57-43cd-9ec8-c852576b52cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Input (Z):\n",
      " [[-0.7]\n",
      " [ 0.1]]\n",
      "\n",
      "Activated Output (f):\n",
      " [[0. ]\n",
      " [0.1]]\n",
      "\n",
      "Predicted Output (y_hat):\n",
      " [[-0.02]]\n",
      "\n",
      "Error (E):\n",
      " [[0.5202]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forward Propagation\n",
    "\n",
    "# Hidden layer linear combination\n",
    "Z = np.dot(W1.T, x) + theta[:2]\n",
    "\n",
    "# ReLU activation\n",
    "f = np.maximum(0, Z)\n",
    "\n",
    "# Output layer (no activation, linear output)\n",
    "y_hat = np.dot(W2, f)\n",
    "\n",
    "# Compute loss (mean squared error)\n",
    "E = 0.5 * (y - y_hat)**2\n",
    "\n",
    "print(\"Hidden Input (Z):\\n\", Z)\n",
    "print(\"\\nActivated Output (f):\\n\", f)\n",
    "print(\"\\nPredicted Output (y_hat):\\n\", y_hat)\n",
    "print(\"\\nError (E):\\n\", E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa35f41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9aa35f41",
    "outputId": "a53b4cb4-016e-407c-e3af-2b8298cfe7be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for W2:\n",
      " [[-0.    -0.102]]\n",
      "\n",
      "Gradient for W1:\n",
      " [[0.    0.204]\n",
      " [0.    0.   ]\n",
      " [0.    0.204]]\n",
      "\n",
      "Updated W2:\n",
      " [[-0.3      -0.199898]]\n",
      "\n",
      "Updated W1:\n",
      " [[ 0.2      -0.300204]\n",
      " [ 0.4       0.1     ]\n",
      " [-0.5       0.199796]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Backward Propagation\n",
    "\n",
    "# Derivative of error w.r.t output\n",
    "dE_dyhat = -(y - y_hat)\n",
    "\n",
    "# Gradient for W2\n",
    "dE_dW2 = dE_dyhat * f.T\n",
    "\n",
    "# Gradient for hidden layer\n",
    "dE_df = dE_dyhat * W2\n",
    "\n",
    "# ReLU derivative (0 if Z <= 0 else 1)\n",
    "dReLU = (Z > 0).astype(float)\n",
    "\n",
    "# Chain rule to get gradient for W1\n",
    "dE_dZ = dE_df.T * dReLU\n",
    "dE_dW1 = np.dot(x, dE_dZ.T)\n",
    "\n",
    "# Weight Updates\n",
    "W2_new = W2 - lr * dE_dW2\n",
    "W1_new = W1 - lr * dE_dW1\n",
    "\n",
    "print(\"Gradient for W2:\\n\", dE_dW2)\n",
    "print(\"\\nGradient for W1:\\n\", dE_dW1)\n",
    "print(\"\\nUpdated W2:\\n\", W2_new)\n",
    "print(\"\\nUpdated W1:\\n\", W1_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8da32",
   "metadata": {
    "id": "23f8da32"
   },
   "source": [
    "\n",
    "### **Result Summary**\n",
    "- **Forward Propagation:** Computed the network output and error.  \n",
    "- **Backward Propagation:** Calculated gradients for each weight using the chain rule.  \n",
    "- **Weight Update:** Adjusted weights using the learning rate (lr = 0.001).  \n",
    "\n",
    "This demonstrates how the network learns by minimizing error through gradient descent.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}